# ğŸ“Š Change Data Capture (CDC) Pipeline using Azure Data Factory & Databricks

This project implements a **Change Data Capture (CDC)** pipeline using:
- **Azure SQL Server** (as the source system),
- **Azure Data Factory** (as orchestrator), and
- **Azure Databricks** (for transformation and storage in Delta Lake format).

---

## ğŸ¯ Objective

To track and process **Insert**, **Update**, and **Delete** operations on source SQL tables using CDC, and store the transformed data in **Delta Lake format** for downstream analytics.

---

## ğŸ—‚ Project Structure

CDC_Project/
â”œâ”€â”€ databricks_notebooks/
â”‚ â””â”€â”€ CDC_SQL_to_Delta.py # PySpark notebook to process CDC and write to Delta
â”œâ”€â”€ sql_scripts/
â”‚ â””â”€â”€ create_tables.sql # SQL script to create & enable CDC on source tables
â”œâ”€â”€ adf_arm_template/
â”‚ â”œâ”€â”€ ARMTemplateForFactory.json # Main ARM template for ADF resources
â”‚ â”œâ”€â”€ ARMTemplateParametersForFactory.json # Deployment parameters (fill in before deploy)
â”‚ â””â”€â”€ linkedTemplates/
â”‚ â””â”€â”€ armTemplate_0.json # Child template for linked deployment
â”œâ”€â”€ .env # Contains ngrok & SQL credentials (optional)
â”œâ”€â”€ README.md # Project documentation


---

## ğŸ›  Technologies Used

- Azure SQL Server
- Azure Data Factory
- Azure Databricks
- Delta Lake (Spark + Parquet)
- PySpark
- ARM Templates (Infra-as-Code)

---
## ğŸ§± Architecture Overview

SQL Server (Local)
â¬‡ (via ngrok + JDBC)
Databricks Notebook (CDC Logic in PySpark)
â¬‡
Delta Table (Databricks / DBFS)
â¬‡ (Optional)
ADF Pipeline (Trigger Notebook via REST API or native activity)


---

## ğŸ” CDC Workflow Steps

1. âœ… Create tables in SQL Server: Customer, Product, Order, Inventory.
2. âœ… Enable and simulate CDC (manually track changes).
3. âœ… Use ngrok to expose your local SQL Server to the public securely.
4. âœ… Create a Databricks Notebook to:
   - Read SQL Server data via JDBC
   - Detect new/updated/deleted rows
   - Write Delta output to `/tmp/customer_cdc_delta`
5. âœ… Validate Delta Lake output in Databricks.
6. âœ… (Optional) Create ADF Pipeline to:
   - Trigger the CDC notebook
   - Schedule the CDC job to run hourly or daily.

---

## âœ… Project Completion Checklist

| Step | Task Description | Status |
|------|------------------|--------|
| 1ï¸âƒ£ | Defined project objective & architecture | âœ… Done |
| 2ï¸âƒ£ | Created SQL Server tables | âœ… Done |
| 3ï¸âƒ£ | Created Self-hosted IR & Linked Services in ADF | âœ… Done |
| 4ï¸âƒ£ | Created Databricks workspace & notebook | âœ… Done |
| 5ï¸âƒ£ | Connected via JDBC using ngrok | âœ… Done |
| 6ï¸âƒ£ | Loaded and transformed data into Delta format | âœ… Done |
| 7ï¸âƒ£ | Tested and validated manually | âœ… Done |
| 8ï¸âƒ£ | Created optional ADF pipeline to trigger notebook | ğŸ” Optional |
| 9ï¸âƒ£ | Scheduled hourly CDC refresh (trigger) | ğŸ” Optional |

---


## ğŸ“ How to Run This Project

1. Clone the repository:
   ```bash
   git clone https://github.com/pragatii01/Celebal_Assignments.git

2. Navigate to the CDC project directory:
   cd Celebal_Assignments/CDC_Project
   
4. Open and configure .env with your:

   - SQL Server credentials

   - JDBC connection string

   - Ngrok auth token (if used)

5. Run the CDC notebook manually in Databricks or via ADF.



ğŸ“ƒ License
This project is submitted as part of the Celebal Internship Assignments and is for educational/demo purposes only.

---

### âœ… Next Step:
Let me know if you want me to:
- Create this structure locally as `.md` files and scripts
- Help you push everything to your GitHub repo

Ready to finish strong! ğŸ’ª
